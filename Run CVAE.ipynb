{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e21a5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optimizer\n",
    "from collections import OrderedDict\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a1c7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:\\SBU-3\\Jupyter-Research\\RSCR\\\\2 Machine Learning\\\\4 CVAE Anar\\\\CVAE Model.ipynb\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a04ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        paths = np.load('C:\\SBU-3\\Jupyter-Research\\RSCR\\Saved Data\\\\555\\\\4_Path_555.npy')\n",
    "        mechs = np.load('C:\\SBU-3\\Jupyter-Research\\RSCR\\Saved Data\\\\555\\\\4_Mec_555_16.npy')\n",
    "        paths = paths.reshape((paths.shape[0], paths.shape[1]*paths.shape[2]))\n",
    "        \n",
    "        if mechs.shape[1] == 16:\n",
    "            mechs = mechs.reshape((mechs.shape[0], mechs.shape[1]))\n",
    "        else:\n",
    "            mechs = mechs.reshape((mechs.shape[0], mechs.shape[1]*mechs.shape[2]))\n",
    "            \n",
    "        self.data = np.hstack((paths, mechs))\n",
    "        random.shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_mech = self.data[idx]\n",
    "        return path_mech[:300].astype(np.float32), path_mech[300:].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72ee0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_eval_cvae(data_loader, model, loss_fn):\n",
    "    model.eval()\n",
    "    overall_loss = 0\n",
    "    kld_loss = 0\n",
    "    mse_loss = 0 \n",
    "    \n",
    "    data_inx = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_inx, data in enumerate(data_loader):\n",
    "            data_inx += 1\n",
    "            path = data[0].to(DEVICE)\n",
    "            joints = data[1].to(DEVICE)\n",
    "            \n",
    "            recon_joints, mu, logvar = model.forward(joints, path)\n",
    "            loss, kld, mse = loss_fn(recon_joints, joints, mu, logvar)\n",
    "            overall_loss += loss.item()\n",
    "            kld_loss += kld.item()\n",
    "            mse_loss += mse.item()\n",
    "        \n",
    "    overall_loss = overall_loss/data_inx\n",
    "    kld_loss = kld_loss/data_inx\n",
    "    mse_loss = mse_loss/data_inx\n",
    "    \n",
    "    return overall_loss, kld_loss, mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece45a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CVAE_Train(train_loader, val_loader, BATCH_SIZE, LR, EPOCH, model):\n",
    "    start_time = time.time()\n",
    "    'Define loss function'\n",
    "    def loss_fn(recon_x, x, mu, logvar):\n",
    "        KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        mse_loss = nn.MSELoss(reduction='mean')(recon_x, x)\n",
    "        return mse_loss+10*KLD, 10*KLD, mse_loss\n",
    "        \n",
    "    'Define optimizer'\n",
    "    optim = optimizer.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = ReduceLROnPlateau(optim, mode='min', factor=0.1, patience=7, threshold=1e-4, cooldown=10)\n",
    "    epochs = EPOCH\n",
    "    \n",
    "    'Print starting loss'\n",
    "    train_loss, train_kld, train_mse = loss_eval_cvae(train_loader, model, loss_fn)\n",
    "    print(\"\\tStart Train Loss:\", train_loss, \"\\tStart KLD:\", train_kld,\"\\tStart MSE:\", train_mse)\n",
    "    #wandb.log({\"Train loss\": train_loss, \"Train KLD\":train_kld, \"Train MSE\": train_mse})\n",
    "    \n",
    "    val_loss, val_kld, val_mse = loss_eval_cvae(val_loader, model, loss_fn)\n",
    "    print(\"\\tStart Val Loss:\", val_loss, \"\\tStart KLD:\", val_kld,\"\\tStart MSE:\", val_mse)\n",
    "    #wandb.log({\"Val loss\":val_loss, \"Val KLD\":val_kld, \"Val MSE\": val_mse})\n",
    "    print('-----------------------------------------------------------------------------------------------------')\n",
    "    'Train'\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_kld = 0\n",
    "        train_mse = 0\n",
    "        \n",
    "        train_inx = 0\n",
    "        for batch_inc, data in enumerate(train_loader):\n",
    "            train_inx += 1\n",
    "            #x = torch.flatten(x[0], start_dim=1)\n",
    "            path = data[0].to(DEVICE)\n",
    "            joints = data[1].to(DEVICE)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            recon_joints, mu, logvar = model.forward(joints, path)\n",
    "            \n",
    "            loss, kld, mse = loss_fn(recon_joints, joints, mu, logvar)\n",
    "            train_loss += loss.item()\n",
    "            train_kld += kld.item()\n",
    "            train_mse += mse.item()\n",
    "            print(train_loss)\n",
    "            \n",
    "            loss.backward()\n",
    "            optim.step()  \n",
    "        train_loss = train_loss/train_inx\n",
    "        train_kld = train_kld/train_inx\n",
    "        train_mse = train_mse/train_inx\n",
    "        #wandb.log({\"Train loss\": train_loss, \"Train KLD\": train_kld, \"Train MSE\": train_mse})\n",
    "      \n",
    "        'Validation in Train'\n",
    "        val_loss, val_kld, val_mse = loss_eval_cvae(val_loader, model, loss_fn)\n",
    "        #wandb.log({\"Val loss\":val_loss, \"Val KLD\":val_kld, \"Val MSE\": val_mse})   \n",
    "        if epoch%50 == 0:\n",
    "            print('-----------------------------------------------------------------------------------------------------')\n",
    "            print(\"\\tEpoch:\", epoch, \"complete!\", \"\\tTime Taken:\", round((time.time() - start_time)/60,2),\"mins\")\n",
    "            print(\"\\tTrain Loss:\", train_loss, \"\\tTrain KLD:\", train_kld,\"\\tTrain MSE:\", train_mse)\n",
    "            print(\"\\tVal Loss:\", val_loss, \"\\tVal KLD:\", val_kld,\"\\tVal MSE:\", val_mse)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "    print(\"Training Complete!\")\n",
    "    \n",
    "\n",
    "    #wandb.finish()\n",
    "    return val_loss, val_kld, val_mse\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3961ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ac2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mechanism_size = 16\n",
    "curve_size = 300\n",
    "batch_size = 1024\n",
    "latent_dim = 2048\n",
    "\n",
    "dataset = VAEDataset()\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.9 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "067d5a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tStart Train Loss: 3.3565389981771157 \tStart KLD: 2.61965755961635 \tStart MSE: 0.736881436083311\n",
      "\tStart Val Loss: 3.3571300765742427 \tStart KLD: 2.6193584203720093 \tStart MSE: 0.7377716626809991\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "3.3590283393859863\n",
      "34.75242471694946\n",
      "43.895023822784424\n",
      "50.140634059906006\n",
      "54.055320024490356\n",
      "56.3044319152832\n",
      "58.159303426742554\n",
      "59.700817823410034\n",
      "60.92042624950409\n",
      "61.87167185544968\n",
      "62.93512183427811\n",
      "63.944606602191925\n",
      "64.78537672758102\n",
      "65.56217581033707\n",
      "66.31423437595367\n",
      "67.01579582691193\n",
      "67.67801773548126\n",
      "68.29881542921066\n",
      "68.9210873246193\n",
      "69.52965837717056\n",
      "70.07686096429825\n",
      "70.59313029050827\n",
      "71.11443883180618\n",
      "71.6447166800499\n",
      "72.14212656021118\n",
      "72.62214922904968\n",
      "73.08666127920151\n",
      "73.54874587059021\n",
      "74.0005242228508\n",
      "74.4421392083168\n",
      "74.8739053606987\n",
      "75.31096720695496\n",
      "75.73340347409248\n",
      "76.14241623878479\n",
      "76.55181014537811\n",
      "76.96958720684052\n",
      "77.37199074029922\n",
      "77.77397730946541\n",
      "78.17441865801811\n",
      "78.57845917344093\n",
      "78.9731265604496\n",
      "79.37420013546944\n",
      "79.76151430606842\n",
      "80.15275657176971\n",
      "80.53852066397667\n",
      "80.92375594377518\n",
      "81.31509116292\n",
      "81.69941732287407\n",
      "82.08072817325592\n",
      "82.46715104579926\n",
      "82.84427618980408\n",
      "83.22135949134827\n",
      "83.60538217425346\n",
      "83.99021598696709\n",
      "84.36119547486305\n",
      "84.74059355258942\n",
      "85.11534124612808\n",
      "85.48403924703598\n",
      "85.86311474442482\n",
      "86.23699468374252\n",
      "86.61101621389389\n",
      "86.98247289657593\n",
      "87.35043874382973\n",
      "87.7329196035862\n",
      "88.11280304193497\n",
      "88.49551293253899\n",
      "88.86482039093971\n",
      "89.2369714975357\n",
      "89.60922130942345\n",
      "89.97714456915855\n",
      "90.35629644989967\n",
      "90.7293848991394\n",
      "91.10632953047752\n",
      "91.4774751663208\n",
      "91.84859630465508\n",
      "92.22506582736969\n",
      "92.59453573822975\n",
      "92.9641124010086\n",
      "93.34107711911201\n",
      "93.7167358994484\n",
      "94.08092603087425\n",
      "94.45725929737091\n",
      "94.82697069644928\n",
      "95.19808116555214\n",
      "95.57265931367874\n",
      "95.94178885221481\n",
      "96.31848067045212\n",
      "96.69371089339256\n",
      "97.0640296638012\n",
      "97.42810416221619\n",
      "97.80005544424057\n",
      "98.16539865732193\n",
      "98.53465411067009\n",
      "98.90249672532082\n",
      "99.27247008681297\n",
      "99.64421153068542\n",
      "100.0166927576065\n",
      "100.38348770141602\n",
      "100.75099295377731\n",
      "101.12844547629356\n",
      "101.49865278601646\n",
      "101.86927330493927\n",
      "102.24143972992897\n",
      "102.60315603017807\n",
      "102.97032871842384\n",
      "103.35011819005013\n",
      "103.72131291031837\n",
      "104.09768798947334\n",
      "104.46450585126877\n",
      "104.83732414245605\n",
      "105.21120110154152\n",
      "105.58239942789078\n",
      "105.95300862193108\n",
      "106.32900422811508\n",
      "106.6980952322483\n",
      "107.07205054163933\n",
      "107.44372054934502\n",
      "107.81957310438156\n",
      "108.19264876842499\n",
      "108.56405112147331\n",
      "108.92920193076134\n",
      "109.2995072901249\n",
      "109.67517358064651\n",
      "110.04393982887268\n",
      "110.4085947573185\n",
      "110.77601617574692\n",
      "111.14188686013222\n",
      "111.50922983884811\n",
      "111.88130071759224\n",
      "112.25520104169846\n",
      "112.62505906820297\n",
      "113.00306808948517\n",
      "113.37517738342285\n",
      "113.75293582677841\n",
      "114.11606323719025\n",
      "114.4854993224144\n",
      "114.85779169201851\n",
      "115.22508004307747\n",
      "115.59556695818901\n",
      "115.96800121665001\n",
      "116.33938017487526\n",
      "116.70526993274689\n",
      "117.0773736834526\n",
      "117.44172450900078\n",
      "117.81198936700821\n",
      "118.18563070893288\n",
      "118.55565506219864\n",
      "118.9258845448494\n",
      "119.29485189914703\n",
      "119.66777709126472\n",
      "120.03918445110321\n",
      "120.40982016921043\n",
      "120.7798179090023\n",
      "121.14883154630661\n",
      "121.51632079482079\n",
      "121.88282024860382\n",
      "122.25591522455215\n",
      "122.62177622318268\n",
      "122.99050885438919\n",
      "123.36481636762619\n",
      "123.73271661996841\n",
      "124.10777696967125\n",
      "124.4727708697319\n",
      "124.84953567385674\n",
      "125.22048193216324\n",
      "125.59348174929619\n",
      "125.95968443155289\n",
      "126.33242467045784\n",
      "126.70633110404015\n",
      "127.07630613446236\n",
      "127.44768780469894\n",
      "127.82470470666885\n",
      "128.19720047712326\n",
      "128.57340735197067\n",
      "128.94612050056458\n",
      "129.3165367245674\n",
      "129.69043722748756\n",
      "130.06675651669502\n",
      "130.439511179924\n",
      "130.81119108200073\n",
      "131.18707594275475\n",
      "131.5627022087574\n",
      "131.93192717432976\n",
      "132.29890954494476\n",
      "132.66919097304344\n",
      "133.0376744568348\n",
      "133.40108451247215\n",
      "133.7662876844406\n",
      "134.13506418466568\n",
      "134.50423246622086\n",
      "134.86822712421417\n",
      "135.2369208931923\n",
      "135.60370048880577\n",
      "135.97391158342361\n",
      "136.3387569785118\n",
      "136.70831882953644\n",
      "137.07859712839127\n",
      "137.44931730628014\n",
      "137.82421898841858\n",
      "138.19169080257416\n",
      "138.56604281067848\n",
      "138.9342626929283\n",
      "139.2990581691265\n",
      "139.66798874735832\n",
      "140.03496557474136\n",
      "140.40403947234154\n",
      "140.77372005581856\n",
      "141.13925966620445\n",
      "141.50962880253792\n",
      "141.88280346989632\n",
      "142.2500822544098\n",
      "142.61627677083015\n",
      "142.98932647705078\n",
      "143.36192780733109\n",
      "143.73928952217102\n",
      "144.10859921574593\n",
      "144.47443068027496\n",
      "144.84191665053368\n",
      "145.2191905081272\n",
      "145.5989845097065\n",
      "145.97748646140099\n",
      "146.3522764146328\n",
      "146.73212537169456\n",
      "147.10866743326187\n",
      "147.47571617364883\n",
      "147.84371840953827\n",
      "148.21066921949387\n",
      "148.5889302790165\n",
      "148.95850339531898\n",
      "149.33297502994537\n",
      "149.6988269984722\n",
      "150.06946277618408\n",
      "150.4407116174698\n",
      "150.80849474668503\n",
      "151.18272066116333\n",
      "151.5528186261654\n",
      "151.9213735461235\n",
      "152.2913199365139\n",
      "152.65995487570763\n",
      "153.0265756547451\n",
      "153.39939641952515\n",
      "153.76817694306374\n",
      "154.14236515760422\n",
      "154.51810961961746\n",
      "154.89146754145622\n",
      "155.26136374473572\n",
      "155.63020452857018\n",
      "156.0061052441597\n",
      "156.37154179811478\n",
      "156.73267823457718\n",
      "157.10682985186577\n",
      "157.4732438325882\n",
      "157.8442643582821\n",
      "158.2177088856697\n",
      "158.57874500751495\n",
      "158.94639784097672\n",
      "159.32112196087837\n",
      "159.68391638994217\n",
      "160.0533185005188\n",
      "160.41610851883888\n",
      "160.79092553257942\n",
      "161.16184532642365\n",
      "161.5337065756321\n",
      "161.90054038167\n",
      "162.27182340621948\n",
      "162.6418098807335\n",
      "163.01738345623016\n",
      "163.3931410908699\n",
      "163.76088652014732\n",
      "164.13259056210518\n",
      "164.5060382783413\n",
      "164.87814128398895\n",
      "165.24932685494423\n",
      "165.62358778715134\n",
      "165.9928931593895\n",
      "166.36560589075089\n",
      "166.7359788119793\n",
      "167.10753855109215\n",
      "167.47876507043839\n",
      "167.8529133796692\n",
      "168.2246964275837\n",
      "168.60052579641342\n",
      "168.97271832823753\n",
      "169.34465363621712\n",
      "169.7122434079647\n",
      "170.08156242966652\n",
      "170.45227459073067\n",
      "170.8278664946556\n",
      "171.19844987988472\n",
      "171.56709963083267\n",
      "171.9334596991539\n",
      "172.3000372350216\n",
      "172.66960614919662\n",
      "173.03740000724792\n",
      "173.40731689333916\n",
      "173.77733078598976\n",
      "174.15002530813217\n",
      "174.51930212974548\n",
      "174.88543245196342\n",
      "175.25552654266357\n",
      "175.63342189788818\n",
      "176.0026606619358\n",
      "176.36357709765434\n",
      "176.7334261238575\n",
      "177.10108038783073\n",
      "177.47063687443733\n",
      "177.84466841816902\n",
      "178.21543416380882\n",
      "178.589045971632\n",
      "178.96169379353523\n",
      "179.33291506767273\n",
      "179.70003852248192\n",
      "180.06774699687958\n",
      "180.4334880709648\n",
      "180.7991867363453\n",
      "181.16430377960205\n",
      "181.53944221138954\n",
      "181.9016181230545\n",
      "182.2716801762581\n",
      "182.6441306769848\n",
      "183.01383656263351\n",
      "183.38347351551056\n",
      "183.76383167505264\n",
      "184.12935423851013\n",
      "184.50272610783577\n",
      "184.86885398626328\n",
      "185.24093201756477\n",
      "185.61073929071426\n",
      "185.9845729470253\n",
      "186.35090345144272\n",
      "186.72972252964973\n",
      "187.09402158856392\n",
      "187.46090239286423\n",
      "187.83182027935982\n",
      "188.2037519812584\n",
      "188.57461178302765\n",
      "188.94439980387688\n",
      "189.31545016169548\n",
      "189.68587037920952\n",
      "190.05456295609474\n",
      "190.4291397333145\n",
      "190.7981307208538\n",
      "191.16827350854874\n",
      "191.54150584340096\n",
      "191.9094067811966\n",
      "192.28085443377495\n",
      "192.64358776807785\n",
      "193.0123289525509\n",
      "193.38408997654915\n",
      "193.75024473667145\n",
      "194.11732563376427\n",
      "194.488685131073\n",
      "194.84864708781242\n",
      "195.22113293409348\n",
      "195.59359675645828\n",
      "195.9585068821907\n",
      "196.3234410583973\n",
      "196.68696841597557\n",
      "197.05897292494774\n",
      "197.4297679066658\n",
      "197.8002881705761\n",
      "198.17638832330704\n",
      "198.545017182827\n",
      "198.92562267184258\n",
      "199.2908152937889\n",
      "199.6595131456852\n",
      "200.0369179546833\n",
      "200.41218116879463\n",
      "200.77261233329773\n",
      "201.13758009672165\n",
      "201.5034190416336\n",
      "201.87036842107773\n",
      "202.23899587988853\n",
      "202.60439494252205\n",
      "202.97189021110535\n",
      "203.3360256254673\n",
      "203.7034700512886\n",
      "204.07309666275978\n",
      "204.44629728794098\n",
      "204.81978118419647\n",
      "205.19225239753723\n",
      "205.56231677532196\n",
      "205.9342737197876\n",
      "206.299160271883\n",
      "206.67094507813454\n",
      "207.04249113798141\n",
      "207.41674080491066\n",
      "207.78528100252151\n",
      "208.16747280955315\n",
      "208.53931471705437\n",
      "208.9115598499775\n",
      "209.28013348579407\n",
      "209.6552094221115\n",
      "210.02409571409225\n",
      "210.39341124892235\n",
      "210.76110592484474\n",
      "211.12885025143623\n",
      "211.49891489744186\n",
      "211.86387214064598\n",
      "212.2269024848938\n",
      "212.59642013907433\n",
      "212.96524873375893\n",
      "213.3324771821499\n",
      "213.70238941907883\n",
      "214.07109367847443\n",
      "214.44159841537476\n",
      "214.80412751436234\n",
      "215.17715480923653\n",
      "215.5462717115879\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\tEpoch: 0 complete! \tTime Taken: 5.5 mins\n",
      "\tTrain Loss: 0.527007999294836 \tTrain KLD: 0.07504058306470905 \tTrain MSE: 0.4519674181209508\n",
      "\tVal Loss: 0.3669112881888514 \tVal KLD: 3.441115444255284e-05 \tVal MSE: 0.3668768762246422\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'early_stopping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_61332\\2878202985.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mEPOCH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mloss_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCVAE_Train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mLOSS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train 1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mLOSS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_61332\\2363123365.py\u001b[0m in \u001b[0;36mCVAE_Train\u001b[1;34m(train_loader, val_loader, BATCH_SIZE, LR, EPOCH, model)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mearly_stopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Early stopping\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'early_stopping' is not defined"
     ]
    }
   ],
   "source": [
    "'Login Wandb'\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"RSCR Journal\",\n",
    "    name = \"4_555_cVAE_A\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"architecture\": \"cVAE_Anar\",\n",
    "    \"dataset\": \"4_Mec_555\",\n",
    "    \"epochs\": 150,\n",
    "    \"batchsize\": 2048,\n",
    "    \"Weight Decay\": 0,\n",
    "    }\n",
    "    \n",
    ")\n",
    "\n",
    "'Train'\n",
    "model_1 = cVAE(mechanism_size, latent_dim, curve_size)\n",
    "model_1 = model_1.to(DEVICE)\n",
    "LR = 1e-3\n",
    "EPOCH = 1\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping('/home/xudeng/XD/data/model', name='4_555_mapper_A_256_2048*6_d0.1')\n",
    "loss_temp = CVAE_Train(train_loader, val_loader, batch_size, LR, EPOCH, model_1)\n",
    "LOSS.append(\"Train 1\")\n",
    "LOSS.append(loss_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b9e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.3.1",
   "language": "python",
   "name": "pytorch1.3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
