{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaab692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optimizer\n",
    "from collections import OrderedDict\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from scipy.optimize import root\n",
    "import math\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:\\SBU-3\\Jupyter-Research\\RSCR\\\\2 Machine Learning\\\\2 VAE Anar\\\\VAE Model.ipynb\"\n",
    "%run \"C:\\SBU-3\\Jupyter-Research\\RSCR\\Functions.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff84813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        paths = np.load('C:\\SBU-3\\Jupyter-Research\\RSCR\\Saved Data\\\\555\\\\5_Path_555.npy')\n",
    "        mechs = np.load('C:\\SBU-3\\Jupyter-Research\\RSCR\\Saved Data\\\\555\\\\5_Mec_555.npy')\n",
    "        paths = paths.reshape((paths.shape[0], paths.shape[1]*paths.shape[2]))\n",
    "        \n",
    "        if mechs.shape[1] == 16:\n",
    "            mechs = mechs.reshape((mechs.shape[0], mechs.shape[1]))\n",
    "        else:\n",
    "            mechs = mechs.reshape((mechs.shape[0], mechs.shape[1]*mechs.shape[2]))\n",
    "            \n",
    "        self.data = np.hstack((paths, mechs))\n",
    "        random.shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #path_mech = {'data':self.data[idx]}\n",
    "        #return path_mech\n",
    "        path_mech = self.data[idx]\n",
    "        return path_mech[:300].astype(np.float32), path_mech[300:].astype(np.float32)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, latent_dim):\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.data = []\n",
    "        for file_name in os.listdir('latents'):\n",
    "            if file_name.endswith('.npy'):\n",
    "                file_path = os.path.join('latents', file_name)\n",
    "                data = np.load(file_path)\n",
    "                self.data.extend(data)\n",
    "                \n",
    "        random.shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_mech = self.data[idx]\n",
    "        return path_mech[:self.latent_dim].astype(np.float32), path_mech[self.latent_dim:].astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e98133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_eval_vae(data_loader, model, loss_fn):\n",
    "    model.eval()\n",
    "    overall_loss = 0\n",
    "    kld_loss = 0\n",
    "    mse_loss = 0 \n",
    "    \n",
    "    data_inx = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_inx, data in enumerate(data_loader):\n",
    "            data_inx += 1\n",
    "            #x = torch.flatten(x[0], start_dim=1)\n",
    "            x = data[0].to(DEVICE)\n",
    "            \n",
    "            xhat, mu, sigma  = model(x)\n",
    "            loss, kld, mse = loss_fn(xhat, x, mu, sigma)\n",
    "            overall_loss += loss.item()\n",
    "            kld_loss += kld.item()\n",
    "            mse_loss += mse.item()\n",
    "        \n",
    "    overall_loss = overall_loss/data_inx\n",
    "    kld_loss = kld_loss/data_inx\n",
    "    mse_loss = mse_loss/data_inx\n",
    "    \n",
    "    return overall_loss, kld_loss, mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f807a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAE_Train(train_loader, val_loader, BATCH_SIZE, LR, EPOCH, model):\n",
    "    start_time = time.time()\n",
    "    'Define loss function'\n",
    "    def loss_fn(x_recon, x, mean, logvar):\n",
    "        kld_loss = -0.5*torch.mean(logvar - logvar.exp() - mean.pow(2) + 1) #kl loss = -0.5*(log(sigma^2) - sigma^2 - mu^2 +1)\n",
    "        #mse_loss = nn.functional.binary_cross_entropy(x_recon, x, reduction='sum')\n",
    "        mse_loss = nn.MSELoss(reduction='sum')(x_recon, x)\n",
    "        #什么时候用mse什么时候用bce？？\n",
    "        return kld_loss+mse_loss, kld_loss, mse_loss\n",
    "    \n",
    "    'Define optimizer'\n",
    "    optim = optimizer.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = ReduceLROnPlateau(optim, mode='min', factor=0.1, patience=7, threshold=1e-4, cooldown=10)\n",
    "    epochs = EPOCH\n",
    "    \n",
    "#     'Print starting loss'\n",
    "#     train_loss, train_kld, train_mse = loss_eval_vae(train_loader, model, loss_fn)\n",
    "#     print(\"\\tStart Train Loss:\", train_loss, \"\\tStart KLD:\", train_kld,\"\\tStart MSE:\", train_mse)\n",
    "#     #wandb.log({\"Train loss\": train_loss, \"Train KLD\":train_kld, \"Train MSE\": train_mse})\n",
    "    \n",
    "#     val_loss, val_kld, val_mse = loss_eval_vae(val_loader, model, loss_fn)\n",
    "#     print(\"\\tStart Val Loss:\", val_loss, \"\\tStart KLD:\", val_kld,\"\\tStart MSE:\", val_mse)\n",
    "#     #wandb.log({\"Val loss\":val_loss, \"Val KLD\":val_kld, \"Val MSE\": val_mse})\n",
    "#     print('-----------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    'Train'\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        train_kld = 0\n",
    "        train_mse = 0\n",
    "        \n",
    "        train_inx = 0\n",
    "        for batch_inc, data in enumerate(train_loader):\n",
    "            train_inx += 1\n",
    "            #x = torch.flatten(x[0], start_dim=1)\n",
    "            x = data[0].to(DEVICE)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            xhat, mu, sigma = model(x)\n",
    "            loss, kld, mse = loss_fn(xhat, x, mu, sigma)\n",
    "            train_loss += loss.item()\n",
    "            train_kld += kld.item()\n",
    "            train_mse += mse.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optim.step()  \n",
    "        train_loss = train_loss/train_inx\n",
    "        train_kld = train_kld/train_inx\n",
    "        train_mse = train_mse/train_inx\n",
    "        #wandb.log({\"Train loss\": train_loss, \"Train KLD\": train_kld, \"Train MSE\": train_mse})\n",
    "        \n",
    "        'Validation in Train'\n",
    "        val_loss, val_kld, val_mse = loss_eval_vae(val_loader, model, loss_fn)\n",
    "        #wandb.log({\"Val loss\":val_loss, \"Val KLD\":val_kld, \"Val MSE\": val_mse})   \n",
    "        \n",
    "        if epoch%50 == 0:\n",
    "            print('-----------------------------------------------------------------------------------------------------')\n",
    "            print(\"\\tEpoch:\", epoch, \"complete!\", \"\\tTime Taken:\", round((time.time() - start_time)/60,2),\"mins\")\n",
    "            print(\"\\tTrain Loss:\", train_loss, \"\\tTrain KLD:\", train_kld,\"\\tTrain MSE:\", train_mse)\n",
    "            print(\"\\tVal Loss:\", val_loss, \"\\tVal KLD:\", val_kld,\"\\tVal MSE:\", val_mse)\n",
    "        \n",
    "        #scheduler.step(val_loss)\n",
    "        #early_stopping(val_loss, model)\n",
    "        #if early_stopping.early_stop:\n",
    "            #print(\"Early stopping\")\n",
    "    print(\"Training Complete!\")\n",
    "    \n",
    "#     'Test'\n",
    "#     print(\"Running Test...\")\n",
    "#     test_loss, test_kld, test_mse = loss_eval_vae(test_loader, model, loss_fn)\n",
    "#     print(\"\\tTest Loss:\", test_loss, \"\\tTest KLD:\",test_kld,\"\\tTest MSE:\",test_mse)\n",
    "    #wandb.log({\"Test loss\": test_loss, \"Test KLD\": test_kld, \"Test MSE\": test_mse})\n",
    "    #wandb.finish()\n",
    "    return test_loss, test_kld, test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab563442",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e12b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "joint_size = 21\n",
    "batch_size = 512\n",
    "latent_dim = 64\n",
    "\n",
    "dataset = VAEDataset()\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Train'\n",
    "model_1 = VAE(input_size, latent_dim)\n",
    "model_1 = model_1.to(DEVICE)\n",
    "LR = 1e-3\n",
    "EPOCH = 10\n",
    "\n",
    "loss_temp = VAE_Train(train_loader, val_loader, batch_size, LR, EPOCH, model_1)\n",
    "LOSS.append(\"Train 1\")\n",
    "LOSS.append(loss_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2523be79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.3.1",
   "language": "python",
   "name": "pytorch1.3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
